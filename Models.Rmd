---
title: "EDA"
author: "Guting Huang"
date: "2022-11-27"
output: 
  pdf_document:
    fig_caption:yes
bibliography:bibliography.bib
---

```{r message=FALSE}
library(rstan)
library(ggplot2)
library(dplyr)
library(tidyr)
library(grid)
library(gridExtra)
library(scales)
library(loo)
library(Metrics)
```

# Introduction

Globally around 800,000 people die from suicide every year. Drop in unemployment at the start and during the COVID-19 pandemic is unprecedented, and there is a noticeable increase in the number of suicides. In this project, our goal is to investigate in the relationship between unemployment rate and suicide rate, trying to make predictions based on unemployment rate for each country. If suicide rates can be predicted by unemployment rate, it can become a useful factor to consider when planning for preventative interventions.

## Problem Description

Our goal is to investigate in the relationship between unemployment rate and suicide rate, trying to make predictions based on unemployment rate for each country.

## Data Description

Our data consists of two datasets retrieved from kaggle.com. The first dataset contains suicide information by year and country from 1985-2016 with 12 variables (country, year, sex, age group, count of suicides, population, suicide rate, country-year composite key, HDI for year, gdp for year \$, gdp per capita, generation). The other dataset includes the unemployment rates by country from 1991-2021.

Since the two dataset have a different time range, we first took their intersection by year and kept only rows whose year are common to both, from 1991-2015. Furthermore, some of the countries do not have complete data for every year in the selected time range, therefore we decided to focus on those European countries that have complete suicide information from 1991-2015. Because of the limited data, we also decided to ignore the year attribute. Finally, the remaining variables that are of interest are country, suicide rate (suicide per 100k), and unemployment rate.

## Main modeling idea

Before going into the models, we should take a look at the data:

```{r}
data <- read.csv("./data/suicide_with_unemploy.csv")
```

```{r}
ggplot(data=data, aes(x=unemployment_rate, y=suicides_per_100k, col=country))+
  geom_point()
```

A quick look at the data suggests that there are no overall trend between the unemployment rate and the suicide rate. However, upon a closer look, we can see that each the unemployment rate and the suicide rate of individual countries seems to have a linear trend. Thus, we decide that we would implement two models: a linear separate model and a linear hierachical model.

In more details, denote the suicide rate of country $j$ as $y_j$, and the corresponding unemployment rate as $x_j$. We want to fit the following model:

$$
y_j = \beta_{0_j} + x_j \cdot \beta_{1_j}
$$

The details of the model and how we choose the priors will be presented in each model section below.

But before going in detailed about the models, we should prepare the data to pass into the Stan program

```{r}
# Add a group indicator column to the data
countries <- unique(data$country)
for (i in 1:length(countries)) {
  data$group[data$country == countries[i]] <- i
}

data_train <- data[data$year < 2015,]  # We leave out data in 2015 to use in prediction
data_valid <- data[data$year == 2015,]

# Data for the stan models
stan_data <- list(
  N = nrow(data_train),
  J = length(countries),
  id = data_train$group,
  x = data_train$unemployment_rate,
  y = data_train$suicides_per_100k
)
```

# Separate model

## Model description

Based on Fountolakis et al's paper \cite{priors}, we can see that the yearly suicide rate are concentrated around 12.83. We also see that the estimated coefficient for the National unemployment rate is 0.005. Thus, it is sensible to say that $\beta_1 \sim N(0, \sigma_{\beta_1})$. We also believe that it is reasonable that increase in 1% of unemployment rate will change the suicide rate by 50%. Thus, the following probability should held for $\beta_1$:

$$
Pr(-6.415 < \beta_1 < 6.415) = 0.99
$$

Solve for $\sigma_{\beta_1}$, we obtain:

$$
\beta_1 \sim N(0, 2.5)
$$

In the same paper, the authors also find out that the intercept for the suicide rate is 1.89. And we believe that the intercept should not have a standard deviation more than 20. Thus, we have the following priors for the intercept:

$$
\beta_0 \sim N(1.89, 20)
$$

For the variance parameter, we do not have any prior knowledge about this. For this reason, we decide to go with a weakly informative prior, more explicitly, a heavy-tailed prior distribution. Because of the heavier tail, it would be safer if the parameters that we choose do not accomodate the true value of the variance parameter. The distribution of choice for this part is Half-Cauchy distribution

$$
\sigma \sim Half-Cauchy(0, 10)
$$

Thus, we have the following separate model:

$$
\begin{align}
\beta_{0_j} &\sim N(1.89, 20) \\
\beta_{1_j} &\sim N(0, 2.5) \\
   \sigma_j &\sim Half-Cauchy(0, 10) \\
        y_j &\sim N(\beta_{0_j} + x_j \cdot \beta_{1_j}, \sigma_j)
\end{align}
$$

## Stan code and running option

Below is the Stan code for the separate model

```{r, echo=FALSE}
writeLines(readLines('stan_model/separate.stan'))
```

This is the running command and its options:

```{r}
sm_sep <- stan(file='stan_model/separate.stan', data=stan_data, iter=2000, refresh=0)
```

## Convergence diagnostics

It seems that there were no warning on convergence for this one, so it is a good sign that the chains are not diverge. But just to be sure, let's visualize the chains for both parameters `\beta_0` and `\beta_1` of Austria.

```{r}
traceplot(sm_sep, pars=c('beta0[1]', 'beta1[1]'))
```

Although there are some chains that behave quite strange, the chains seem to be converged. Let us also look at the `\hat{R}` values

```{r}
df <- as.data.frame(summary(sm_sep)$summary)
head(df, 34)
```


With the $\hat{R}$ values are close to 1 and below the recommended value of 1.05, we can be say that the chains are indeed converged well.

## Posterior predictive check

We can draw the fitted line onto the data points to assess the posterior predictive of the model. In this part, we only choose the median value of each `\beta_0` and `\beta_1` to make the figure looks a bit nicer.

```{r, fig.width=15, fig.height=10, fig.cap="\\label{fitted_sep}The fitting of the linear model to the data points", warning=FALSE, echo=FALSE}
J <- length(countries)
hex_code <- hue_pal()(J)

plot <- ggplot() + 
  geom_point(data=data_train, aes(x=unemployment_rate, y=suicides_per_100k, col=country))


draws <- rstan::extract(sm_sep)


for(j in 1:J) {
  beta0 <- quantile(draws$beta0[,j], c(0.05, 0.5, 0.95))[['50%']]
  beta1 <- quantile(draws$beta1[,j], c(0.05, 0.5, 0.95))[['50%']]
  df_country <- data.frame(unemployment=c(0:30))
  df_country$suicides <- beta0 + beta1 * df_country$unemployment
  plot <- plot + geom_line(data=df_country, aes(x=unemployment, y=suicides), col=hex_code[j])
}

plot
```
The plot can be a bit confusing to look at, let's break down the plot into four subplots, each have 4-5 countries:

```{r, fig.width=12, fig.height=8, fig.cap="\\label{break_down_sep}Break down of Figure \ref{fitted_sep}", echo=FALSE}
# The first four countries
hex_code_4 <- hue_pal()(4)
plot1 <- ggplot() +
  geom_point(data=data_train[data_train$country %in% countries[1:4], ], aes(x=unemployment_rate, y=suicides_per_100k, col=country)) + 
  labs(x='Unemployment rate', y ='Suicide rate')
for(j in 1:4) {
  beta0 <- quantile(draws$beta0[,j], c(0.05, 0.5, 0.95))[['50%']]
  beta1 <- quantile(draws$beta1[,j], c(0.5))[['50%']]
  df_country <- data.frame(unemployment=(0:30))
  df_country$suicides <- beta0 + beta1 * df_country$unemployment
  plot1 <- plot1 + geom_line(data=df_country, aes(x=unemployment, y=suicides), col=hex_code_4[j])
}


# The next four countries
plot2 <- ggplot() +
  geom_point(data=data_train[data_train$country %in% countries[5:8], ], aes(x=unemployment_rate, y=suicides_per_100k, col=country)) + 
  labs(x='Unemployment rate', y ='Suicide rate')
hex_code_4 <- hue_pal()(4)
for(j in 5:8) {
  beta0 <- quantile(draws$beta0[,j], c(0.05, 0.5, 0.95))[['50%']]
  beta1 <- quantile(draws$beta1[,j], c(0.5))[['50%']]
  df_country <- data.frame(unemployment=(0:30))
  df_country$suicides <- beta0 + beta1 * df_country$unemployment
  plot2 <- plot2 + geom_line(data=df_country, aes(x=unemployment, y=suicides), col=hex_code_4[j-4])
}


# The next four countries
plot3 <- ggplot() +
  geom_point(data=data_train[data_train$country %in% countries[9:12], ], aes(x=unemployment_rate, y=suicides_per_100k, col=country)) + 
  labs(x='Unemployment rate', y ='Suicide rate')
hex_code_4 <- hue_pal()(4)
for(j in 9:12) {
  beta0 <- quantile(draws$beta0[,j], c(0.05, 0.5, 0.95))[['50%']]
  beta1 <- quantile(draws$beta1[,j], c(0.5))[['50%']]
  df_country <- data.frame(unemployment=(0:30))
  df_country$suicides <- beta0 + beta1 * df_country$unemployment
  plot3 <- plot3 + geom_line(data=df_country, aes(x=unemployment, y=suicides), col=hex_code_4[j-8])
}


# The final five countries
plot4 <- ggplot() +
  geom_point(data=data_train[data_train$country %in% countries[13:17], ], aes(x=unemployment_rate, y=suicides_per_100k, col=country)) + 
  labs(x='Unemployment rate', y ='Suicide rate')
hex_code_4 <- hue_pal()(5)
for(j in 13:17) {
  beta0 <- quantile(draws$beta0[,j], c(0.05, 0.5, 0.95))[['50%']]
  beta1 <- quantile(draws$beta1[,j], c(0.5))[['50%']]
  df_country <- data.frame(unemployment=(0:30))
  df_country$suicides <- beta0 + beta1 * df_country$unemployment
  plot4 <- plot4 + geom_line(data=df_country, aes(x=unemployment, y=suicides), col=hex_code_4[j-12])
}

grid.arrange(plot1, plot2, plot3, plot4, nrow=2)

```

It can be seen from Figure \ref{fitted_hie} and Figure \ref{break_down_hie} that the line seems to be a good fit for most of the countries.

Another method to assess the predictive performance of the model is to perform leave-one-out cross-validation (CV-LOO). Below is the code to perform PSIS-LOO and visualize $\hat{k}$:

```{r}
log_lik_sep <- extract_log_lik(sm_sep, merge_chains = FALSE)
r_eff_sep <- relative_eff(exp(log_lik_sep), cores=4)
loo_sep <- loo(log_lik_sep, r_eff=r_eff_sep, cores=4)
```

```{r, fig.cap="\\label{pareto_k_sep}$\hat{k}$ values of the separate model"}
pareto_k_sep <- as.data.frame(pareto_k_values(loo_sep))
ggplot(pareto_k_sep, aes(y=pareto_k_values(loo_sep), x=1:(425-17))) +
geom_point() +
geom_hline(yintercept=0.7, linetype="dashed") +
geom_hline(yintercept=1) +
ylim(-0.25, 1.25) +
labs(titles='Pareto k values for observations of the separate model',
x='Observation', y='Pareto k value')
```

As can be seen in Figure \ref{pareto_k_sep}, some of the $\hat{k}$ values are higher than 0.7 and even one value higher than 1. This indicate that our model is likely to be optimistic.

## Predictive performance assessment

Next, let us predict the suicide rate in 2015. To get the value for $\beta_1$ and $\beta_0$, we will just simply take the mean of the corresponding sample group.

```{r}
pred <- c()


for (j in 1:J) {
  beta0_mean <- mean(draws$beta0[,j])
  beta1_mean <- mean(draws$beta1[,j])
  pred <- c(pred, data_valid$unemployment_rate[j]*beta1_mean + beta0_mean)
}
data_valid$pred_sep <- pred


ggplot() +
  geom_point(data=data_valid, aes(x=unemployment_rate, y=suicides_per_100k, col='blue')) +
  geom_point(data=data_valid, aes(x=unemployment_rate, y=pred_sep, col='red')) +
  theme(legend.title = element_blank())+
  scale_color_manual(labels = c("True value", "Predicted value"), values = c("blue", "red"))
```

From the plot, we can see that many of the predicted data points are far from their true value. To get a better picture, let us calculate the RSME of the prediction:

```{r}
rmse(data_valid$suicides_per_100k, data_valid$pred_sep)
```
Given the scale of the data, an RMSE of 4 is quite large. One reason we can think of is the number of validation data. For each of the country, we only have one validation data point. Thus, this error might not convey the complete message.

## Prior sensitivity analysis

To wrap up the analysis of the separate model, let us do a sensitivity analysis. We can make a new separate model by fitting a very weak prior to the parameters. In more details, we will change the priors for $\beta_0$ and $\beta_1$ to $N(0, 200)$, and the $\sigma$ parameters to $Half-Cauchy(0, 50)$. Below is the Stan code for the new model:

```{r, echo=FALSE}
writeLines(readLines('stan_model/separate_2.stan'))
```

```{r}
sm_sep_2 <- stan(file='stan_model/separate_2.stan', data=stan_data, iter=2000, refresh=0)
head(as.data.frame(summary(sm_sep_2)$summary), 34)
```

And below is the summary of the original separate model

```{r}
head(df, 34)
```

With a visual assessment, the difference in some of the $\beta_0$ of the two models is quite significant, while the difference in $\beta_1$ is miniscule. So it is safe to say that our model is relatively not sensitive to prior choice.


# Hierachical model

## Model description

While the performance of the separate model seems to be good, we want to go a bit further. It is quite natural to think that the slopes and intercepts of different countries can come from a common population distribution. For that reason, we dicided to fit a hierachical model to our data set.

With visual assessment of the data, it seems that all of the countries share the same intercept. That is why we only impose a hierachical structure on the slope parameter.

We use the same reasoning as above to obtain a prior $N(1.89, 20)$ for intercept $\beta_0$ and a $Half-Cauchy(0, 10)$ prior for the variance $\sigma$.

For the slope $\beta_1$, this time, we want to have a prior as follow:

$$
\beta_{1_j} \sim N(\mu_{1_j}, \sigma_{1_j})
$$

Similar to before, we believe the mean of the $\beta_1$ should be centered around 0. Thus, the parameter $\mu_{1_j}$ should also be centered around 0. We also select the standard deviation value of 30 for $\mu_{1_j}$ to reflect that we are not very confident about this value. For $\sigma_{1_j}$, we select a $Half-Cauchy(0, 10)$ distribution as a weakly informative distribution for this value.

The complete hierachical model:

$$
\begin{align}
   \mu_{1_j} &\sim N(0, 10) \\
\sigma_{1_j} &\sim Half-Cauchy(0, 10) \\
 \beta_{1_j} &\sim N(\mu_{1_j}, \sigma_{1_j}) \\
     \beta_0 &\sim N(1.89, 20) \\
      \sigma &\sim Half-Cauchy(0, 10) \\
      y_{ij} &\sim N(\beta_0 + \beta_{1_j} \cdot x_{ij}, \sigma)
\end{align}
$$

## Stan code and running options

Below is the Stan code for the hierachical model:

```{r, echo=FALSE}
writeLines(readLines('stan_model/linear_hierachical.stan'))
```

Our setting to run the file was as below:

```{r}
sm_hie <- stan(file='stan_model/linear_hierachical.stan', data=stan_data, iter=2000, refresh=0)
```

## Convergence diagnostics

It seems that there were no warning on convergence for this one, so it is a good sign that the chains are not diverge. In the first run of the code, there was actually a divergence warning. But when we increase the iteration, the warning was gone. But just to be sure, let's visualize the chains for `\beta_0` and `\beta_1` of Austria

```{r, fig.cap="\\label{trace_hie}The chains of the intercept and the slope of Austria group"}
traceplot(sm_hie, pars=c('beta0', 'beta1[1]'))
```

With visual inspection, it seems that the chains converged quite well. But we want to take a step further and investigate the $\hat{R}$ to detect any potential divergence

```{r}
df <- as.data.frame(summary(sm_hie)$summary)
head(df, 20)
```

The $\hat{R}$ values are all close to 1, and are below the recommended value of 1.05. Combined with Figure \ref{trace_hie}, this is indicates that the chains have converged well.

## Posterior predictive check

We can draw the fitted line onto the data points to assess the posterior predictive of the model. In this part, we only choose the median value `\beta_0` and each `\beta_1` to make the figure looks a bit nicer.

```{r, fig.width=15, fig.height=10, fig.cap="\\label{fitted_hie}The fitting of the linear model to the data points", warning=FALSE, echo=FALSE}
hex_code <- hue_pal()(J)

plot <- ggplot() +
  geom_point(data=data_train, aes(x=unemployment_rate, y=suicides_per_100k, col=country)) + 
  labs(x='Unemployment rate', y ='Suicide rate') +
  ylim(c(-10,50))


draws <- rstan::extract(sm_hie)
beta0 <- quantile(draws$beta0, c(0.5))[['50%']]

for(j in 1:J) {
  beta1 <- quantile(draws$beta1[,j], c(0.5))[['50%']]
  df_country <- data.frame(unemployment=(0:30))
  df_country$suicides <- beta0 + beta1 * df_country$unemployment
  plot <- plot + geom_line(data=df_country, aes(x=unemployment, y=suicides), col=hex_code[j])
}



plot

```

Again, let's break down the plot into four subplots of 4-5 countries:

```{r, fig.width=12, fig.height=8, fig.cap="\\label{break_down_hie}Break down of Figure \ref{fitted_hie}", echo=FALSE}
# The first four countries
hex_code_4 <- hue_pal()(4)
plot1 <- ggplot() +
  geom_point(data=data_train[data_train$country %in% countries[1:4], ], aes(x=unemployment_rate, y=suicides_per_100k, col=country)) + 
  labs(x='Unemployment rate', y ='Suicide rate')
for(j in 1:4) {
  beta1 <- quantile(draws$beta1[,j], c(0.5))[['50%']]
  df_country <- data.frame(unemployment=(0:30))
  df_country$suicides <- beta0 + beta1 * df_country$unemployment
  plot1 <- plot1 + geom_line(data=df_country, aes(x=unemployment, y=suicides), col=hex_code_4[j])
}


# The next four countries
plot2 <- ggplot() +
  geom_point(data=data_train[data_train$country %in% countries[5:8], ], aes(x=unemployment_rate, y=suicides_per_100k, col=country)) + 
  labs(x='Unemployment rate', y ='Suicide rate')
hex_code_4 <- hue_pal()(4)
for(j in 5:8) {
  beta1 <- quantile(draws$beta1[,j], c(0.5))[['50%']]
  df_country <- data.frame(unemployment=(0:30))
  df_country$suicides <- beta0 + beta1 * df_country$unemployment
  plot2 <- plot2 + geom_line(data=df_country, aes(x=unemployment, y=suicides), col=hex_code_4[j-4])
}


# The next four countries
plot3 <- ggplot() +
  geom_point(data=data_train[data_train$country %in% countries[9:12], ], aes(x=unemployment_rate, y=suicides_per_100k, col=country)) + 
  labs(x='Unemployment rate', y ='Suicide rate')
hex_code_4 <- hue_pal()(4)
for(j in 9:12) {
  beta1 <- quantile(draws$beta1[,j], c(0.5))[['50%']]
  df_country <- data.frame(unemployment=(0:30))
  df_country$suicides <- beta0 + beta1 * df_country$unemployment
  plot3 <- plot3 + geom_line(data=df_country, aes(x=unemployment, y=suicides), col=hex_code_4[j-8])
}


# The final five countries
plot4 <- ggplot() +
  geom_point(data=data_train[data_train$country %in% countries[13:17], ], aes(x=unemployment_rate, y=suicides_per_100k, col=country)) + 
  labs(x='Unemployment rate', y ='Suicide rate')
hex_code_4 <- hue_pal()(5)
for(j in 13:17) {
  beta1 <- quantile(draws$beta1[,j], c(0.5))[['50%']]
  df_country <- data.frame(unemployment=(0:30))
  df_country$suicides <- beta0 + beta1 * df_country$unemployment
  plot4 <- plot4 + geom_line(data=df_country, aes(x=unemployment, y=suicides), col=hex_code_4[j-12])
}

grid.arrange(plot1, plot2, plot3, plot4, nrow=2)

```

It can be seen from Figure \ref{fitted_hie} and Figure \ref{break_down_hie} that the line seems to be a good fit for most of the countries.

Another method to assess the predictive performance of the model is to perform leave-one-out cross-validation (CV-LOO). Below is the code to perform PSIS-LOO and visualize $\hat{k}$:

```{r}
log_lik_hie <- extract_log_lik(sm_hie, merge_chains=F)
r_eff_hie <- relative_eff(exp(log_lik_hie), cores=4)
loo_hie <- loo(log_lik_hie, r_eff=r_eff_hie, cores=4)
```

```{r, echo=FALSE}
pareto_k_hie <- as.data.frame(pareto_k_values(loo_hie))
ggplot(pareto_k_hie, aes(y=pareto_k_values(loo_hie), x=1:(425-17))) +
geom_point() +
geom_hline(yintercept=0.7, linetype="dashed") +
geom_hline(yintercept=1) +
ylim(-0.25, 1) +
labs(titles='Pareto k values for observations of the hierachical model',
x='Observation', y='Pareto k value')
```

As shown in the plot, all $\hat{k}$ value are under 0.7. This indicates that the model is reliable.

## Predictive performance assessment

Let's try to visualize the prediction for year 2015. To get the value for $\beta_1$ and $\beta_0$, we will just simply take the mean of the corresponding sample group

```{r, echo=FALSE}
pred <- c()

beta0_mean <- mean(draws$beta0)

for (j in 1:J) {
  beta1_mean <- mean(draws$beta1[,j])
  pred <- c(pred, data_valid$unemployment_rate[j]*beta1_mean + beta0_mean)
}
data_valid$pred_hie <- pred


ggplot() +
  geom_point(data=data_valid, aes(x=unemployment_rate, y=suicides_per_100k, col='blue')) +
  geom_point(data=data_valid, aes(x=unemployment_rate, y=pred_hie, col='red')) +
  theme(legend.title = element_blank())+
  scale_color_manual(labels = c("True value", "Predicted value"), values = c("blue", "red"))
```

From the plot, we can see that for most of the data point, the predicted value seems a bit far away from the true value. But we can check a bit closer using RMSE:

```{r}
rmse(data_valid$suicides_per_100k, data_valid$pred_hie)
```

We can see that the RMSE here is still large, but it is a bit smaller than the separate model. Again, we hypothesized that one reason for this is due to the lack of validating samples. 

## Prior sensitivity analysis

Again, to wrap up the analysis of the hierachical model, let us do a prior sensitivity analysis. Again, let us fit a very weak priors to all of the parameters. For $\mu_1$ and $\beta_0$, I will replace the old prior with a $N(0, 200)$ distribution, and for all of the $\sigma_1$ and $\sigma$, I will replace them with a $Half-Cauchy(0, 50)$. Below is the Stan code.

```{r, echo=FALSE}
writeLines(readLines('stan_model/linear_hierachical_2.stan'))
```

```{r}
sm_hie_2 <- stan(file='stan_model/linear_hierachical_2.stan', data=stan_data, iter=2000, refresh=0)
head(as.data.frame(summary(sm_hie_2)$summary), 20)
```

And below is the summary of the original hierachical model

```{r}
head(df, 20)
```

A visual comparison of the two models shows that the value of the $\beta_1$ and $\beta_0$ are very close to each other. This suggests that our model is not sensitive to changes in priors.

# Model comparision

From previous parts, it can be seen that the hierachical model seems to be a better choice. It has a slightly smaller RMSE when predicting new values, and also less susceptible to optimistic due to PSIS analysis. But to be sure, we will perform a model comparision with `loo_compare`

```{r}
loo_compare(list('separate'=loo_sep, 'hierachical'=loo_hie))
```

This is a surprise for us. The elpd difference between the separate and hierachical model is very large. It seems that according to PSIS-LOO, the best model is the separate model.